# -*- coding: utf-8 -*-
"""lab 04 aga

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HYMF-MLH-ZoUu1Dw6sWSE6Q8bRg8-mhg
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class RBM(nn.Module):
    def __init__(self, visible_units, hidden_units, k=1, learning_rate=0.01):
        super(RBM, self).__init__()
        self.visible_units = visible_units
        self.hidden_units = hidden_units
        self.k = k  # Contrastive Divergence steps
        self.lr = learning_rate

        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.1)
        self.h_bias = nn.Parameter(torch.zeros(hidden_units))
        self.v_bias = nn.Parameter(torch.zeros(visible_units))

    def sample_h(self, v):
        p_h_given_v = torch.sigmoid(F.linear(v, self.W, self.h_bias))
        return p_h_given_v, torch.bernoulli(p_h_given_v)

    def sample_v(self, h):
        p_v_given_h = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))
        return p_v_given_h, torch.bernoulli(p_v_given_h)

    def contrastive_divergence(self, v):
        v0 = v
        ph0, h0 = self.sample_h(v0)

        v_k = v0
        for _ in range(self.k):
            _, h_k = self.sample_h(v_k)
            _, v_k = self.sample_v(h_k)

        phk, hk = self.sample_h(v_k)

        loss = torch.mean((v0 - v_k) ** 2)  # Mean Squared Error Loss
        loss.backward()  # Compute gradients
        return v_k

    def forward(self, v):
        return self.sample_h(v)[0]

    def train_rbm(self, data, batch_size=64, epochs=10):
        optimizer = optim.SGD(self.parameters(), lr=self.lr)
        dataset = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            loss_epoch = 0
            for batch in dataset:
                batch = batch.view(-1, self.visible_units)
                optimizer.zero_grad()  # Reset gradients
                v_k = self.contrastive_divergence(batch)
                optimizer.step()  # Update weights
                loss_epoch += torch.mean((batch - v_k) ** 2).item()
            print(f"Epoch {epoch+1}, Loss: {loss_epoch/len(dataset)}")

if __name__ == "__main__":
    visible_units = 784  # Example for MNIST-like data
    hidden_units = 256
    data = torch.rand(1000, visible_units)  # Random training data

    rbm = RBM(visible_units, hidden_units)
    print("\nTraining RBM...")
    rbm.train_rbm(data)

    # Testing reconstruction
    test_sample = torch.rand(1, visible_units)
    print("\nOriginal Input (first 10 values):", test_sample[0][:10].numpy())

    reconstructed_sample = rbm.sample_v(rbm.sample_h(test_sample)[1])[1]
    print("Reconstructed Output (first 10 values):", reconstructed_sample[0][:10].detach().numpy())

    # If you want to visualize the entire vector, remove [:10] slicing

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class RBM(nn.Module):
    def __init__(self, visible_units, hidden_units, k=1, learning_rate=0.01):
        super(RBM, self).__init__()
        self.visible_units = visible_units
        self.hidden_units = hidden_units
        self.k = k  # Contrastive Divergence steps
        self.lr = learning_rate

        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.1)
        self.h_bias = nn.Parameter(torch.zeros(hidden_units))
        self.v_bias = nn.Parameter(torch.zeros(visible_units))

    def sample_h(self, v):
        p_h_given_v = torch.sigmoid(F.linear(v, self.W, self.h_bias))
        return p_h_given_v, torch.bernoulli(p_h_given_v)

    def sample_v(self, h):
        p_v_given_h = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))
        return p_v_given_h, torch.bernoulli(p_v_given_h)

    def contrastive_divergence(self, v):
        v0 = v
        ph0, h0 = self.sample_h(v0)

        v_k = v0
        for _ in range(self.k):
            _, h_k = self.sample_h(v_k)
            _, v_k = self.sample_v(h_k)

        phk, hk = self.sample_h(v_k)

        loss = torch.mean((v0 - v_k) ** 2)  # Mean Squared Error Loss
        loss.backward()  # Compute gradients
        return v_k

    def forward(self, v):
        return self.sample_h(v)[0]

    def train_rbm(self, data, batch_size=64, epochs=10):
        optimizer = optim.SGD(self.parameters(), lr=self.lr)
        dataset = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            loss_epoch = 0
            for batch in dataset:
                batch = batch.view(-1, self.visible_units)
                optimizer.zero_grad()  # Reset gradients
                v_k = self.contrastive_divergence(batch)
                optimizer.step()  # Update weights
                loss_epoch += torch.mean((batch - v_k) ** 2).item()
            print(f"Epoch {epoch+1}, Loss: {loss_epoch/len(dataset)}")

if __name__ == "__main__":
    visible_units = 100  # Example for MNIST-like data
    hidden_units = 50
    data = torch.rand(1000, visible_units)  # Random training data

    rbm = RBM(visible_units, hidden_units)
    print("Training RBM...")
    rbm.train_rbm(data)

    # Testing reconstruction
    test_sample = torch.rand(1, visible_units)
    reconstructed_sample = rbm.sample_v(rbm.sample_h(test_sample)[1])[1]
    print("\nOriginal Input:", test_sample.numpy())
    print("Reconstructed Output:", reconstructed_sample.detach().numpy())